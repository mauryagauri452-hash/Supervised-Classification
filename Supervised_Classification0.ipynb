{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Classification:\n",
        "\n",
        "\n",
        "1. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "- Information Gain is a metric used in decision trees to measure how well a feature splits the data into pure subsets. It is defined as the reduction in entropy after a dataset is split based on a feature. In other words, it quantifies how much ‚Äúinformation‚Äù a feature gives about the class labels.\n",
        "\n",
        "Decision trees use Information Gain to select the best attribute to split on at each node. The attribute with the highest Information Gain is chosen because it produces the most homogeneous child nodes, improving classification accuracy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DFSWbNWRL8U_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "- Gini Impurity measures how often a randomly chosen sample would be misclassified if labels were assigned according to class proportions.\n",
        "\n",
        "Formula:\n",
        "1\n",
        "‚àí\n",
        "‚àë\n",
        "ùëù\n",
        "ùëñ\n",
        "2\n",
        "1‚àí‚àëp\n",
        "i\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Faster to compute (no logarithms).\n",
        "\n",
        "Often used in CART decision trees.\n",
        "\n",
        "Entropy measures the amount of uncertainty or randomness in the data.\n",
        "\n",
        "Formula:\n",
        "‚àí\n",
        "‚àë\n",
        "ùëù\n",
        "ùëñ\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "ùëù\n",
        "ùëñ\n",
        "‚àí‚àëp\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Slower to compute but more theoretically grounded (from information theory).\n",
        "\n",
        "Used in ID3, C4.5 trees.\n",
        "\n",
        "In simple terms: Gini is computationally simpler, while Entropy gives a more accurate measure of disorder."
      ],
      "metadata": {
        "id": "bU-aIMtCMrIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "- Pre-pruning (also called early stopping) is a technique used to stop a decision tree from growing too deep and overfitting the training data. The tree stops splitting a node based on certain conditions, such as:\n",
        "\n",
        "Minimum number of samples required to split\n",
        "\n",
        "Maximum depth of the tree\n",
        "\n",
        "Minimum improvement in impurity\n",
        "\n",
        "Minimum leaf size\n",
        "\n",
        "Pre-pruning helps control model complexity and improves generalization to unseen data.\n"
      ],
      "metadata": {
        "id": "aLc9g6blNj3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train a Decision Tree Classifier using Gini\n",
        "#Impurity as the criterion and print the feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train Decision Tree with Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "for name, importance in zip(data.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ob-1MOWOorh",
        "outputId": "175d77c1-05a9-476f-e2f8-dfde62a4e7ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm): 0.026666666666666658\n",
            "sepal width (cm): 0.0\n",
            "petal length (cm): 0.05072262479871173\n",
            "petal width (cm): 0.9226107085346216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "- A Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. SVM attempts to find the best separating hyperplane that maximizes the margin between classes. The points closest to the margin are called support vectors, and they determine the decision boundary.\n",
        "\n",
        "SVM works well in high-dimensional spaces and can handle both linear and non-linear decision boundaries using kernels."
      ],
      "metadata": {
        "id": "acB7zuTzPYUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the Kernel Trick in SVM?\n",
        "\n",
        "- The Kernel Trick is a technique that allows SVMs to learn non-linear decision boundaries by implicitly mapping data into a higher-dimensional space without computing the transformation explicitly.\n",
        "\n",
        "Common kernels:\n",
        "\n",
        "Linear\n",
        "\n",
        "Polynomial\n",
        "\n",
        "RBF (Gaussian)\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "The Kernel Trick makes SVMs highly powerful for complex datasets."
      ],
      "metadata": {
        "id": "hrIRh_ZqPf7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "#kernels on the Wine dataset, then compare their accuracies ?\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Linear SVM\n",
        "clf_linear = SVC(kernel='linear')\n",
        "clf_linear.fit(X_train, y_train)\n",
        "acc_linear = accuracy_score(y_test, clf_linear.predict(X_test))\n",
        "\n",
        "# RBF SVM\n",
        "clf_rbf = SVC(kernel='rbf')\n",
        "clf_rbf.fit(X_train, y_train)\n",
        "acc_rbf = accuracy_score(y_test, clf_rbf.predict(X_test))\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", acc_linear)\n",
        "print(\"RBF Kernel Accuracy:\", acc_rbf)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeYbdrpzQC25",
        "outputId": "27a307e2-98ed-43bf-d91c-ce36725a8d52"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9814814814814815\n",
            "RBF Kernel Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "- Naive Bayes is a probabilistic classification algorithm based on Bayes‚Äô Theorem, which predicts class membership probabilities based on feature likelihoods.\n",
        "\n",
        "It is called ‚ÄúNaive‚Äù because it assumes that all features are independent, which is rarely true in real-world data. Despite this unrealistic assumption, Naive Bayes works surprisingly well in many applications like text classification and spam detection."
      ],
      "metadata": {
        "id": "pARPLNhxQa9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve\n",
        "Bayes, and Bernoulli Na√Øve Bayes\n",
        "\n",
        "- The three Na√Øve Bayes models differ based on the type of data they are designed for:\n",
        "\n",
        "1. Gaussian Na√Øve Bayes\n",
        "\n",
        "Used for continuous numerical features.\n",
        "\n",
        "Assumes that features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Suitable for datasets like iris measurements, medical data, sensor readings, etc.\n",
        "\n",
        "2. Multinomial Na√Øve Bayes\n",
        "\n",
        "Used for count-based features, such as word frequencies.\n",
        "\n",
        "Commonly applied in text classification, spam filtering, NLP tasks.\n",
        "\n",
        "Assumes feature values represent counts (non-negative integers).\n",
        "\n",
        "3. Bernoulli Na√Øve Bayes\n",
        "\n",
        "Used for binary features (0/1).\n",
        "\n",
        "Each feature indicates presence/absence of a word or event.\n",
        "\n",
        "Works well for text classification when using binary bag-of-words models.\n",
        "\n",
        "Summary:\n",
        "\n",
        "Gaussian NB ‚Üí continuous data\n",
        "\n",
        "Multinomial NB ‚Üí count data\n",
        "\n",
        "Bernoulli NB ‚Üí binary data"
      ],
      "metadata": {
        "id": "uQZHRzz3Q-tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"10.Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naive Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on Breast Cancer Dataset:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoqgcpStRdtr",
        "outputId": "9d1ef4e6-28cb-4a79-e554-d5dbb1bdcc94"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Breast Cancer Dataset: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}